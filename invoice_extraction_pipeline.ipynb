{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b07781-f80d-4f8e-ab26-e7b7525d4641",
   "metadata": {},
   "source": [
    "# üßæ Invoice Information Extraction using OCR + LayoutLMv3\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Problem Statement\n",
    "\n",
    "Manual data entry from invoices is time-consuming, error-prone, and inefficient.  \n",
    "Businesses receive invoices in various formats and need to extract key details like invoice number, dates, client details, and payment information to process and record them.\n",
    "\n",
    "> The challenge is to extract structured information from **unstructured invoice images or PDFs**, regardless of layout or format.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "To build an **end-to-end pipeline** that can:\n",
    "- üß† Automatically extract structured fields from invoice images\n",
    "- üßæ Identify fields like `Invoice Number`, `Invoice Date`, `Seller Info`, `Client Info`, `Items`, `VAT`, `Net Total`, and `Gross Total`\n",
    "- üîÑ Combine **OCR-based rule extraction** and **deep learning (LayoutLMv3)** for accuracy and scalability\n",
    "- üíæ Output the extracted data in structured formats (JSON, CSV)\n",
    "- üìä Make the solution scalable for different invoice layouts and easy to extend for other documents (receipts, forms, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "üì¶ This project showcases both traditional and modern AI techniques for solving a **real-world document automation problem**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0c0bcd-4525-49fc-a96f-bdec8f89ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a772bb-1287-495f-96e1-2fb00f502bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Extracted Invoice Text:\n",
      "========================================\n",
      "Invoice no: 17045625\n",
      "\n",
      "Date of issue:\n",
      "\n",
      "Seller:\n",
      "\n",
      "Bass-Petersen\n",
      "88013 Keith Orchard\n",
      "Port Jenniferfurt, ID 01419\n",
      "\n",
      "Tax Id: 903-94-7610\n",
      "IBAN: GB43AYYU13392404742725\n",
      "\n",
      "ITEMS\n",
      "No. Description\n",
      "1. Cell: A Novel by Stephen King\n",
      "\n",
      "2. A History of the Indians of the\n",
      "United States (The\n",
      "\n",
      "3. Quilts of Illusion\n",
      "\n",
      "SUMMARY\n",
      "\n",
      "Total\n",
      "\n",
      "11/01/2017\n",
      "\n",
      "Qty uM\n",
      "4,00 each\n",
      "1,00 each\n",
      "1,00 each\n",
      "\n",
      "VAT [%]\n",
      "\n",
      "10%\n",
      "\n",
      "Client:\n",
      "\n",
      "Davidson-Martinez\n",
      "36262 Walters Vista\n",
      "Evansstad, ND 36644\n",
      "\n",
      "Tax Id: 913-98-2620\n",
      "\n",
      "Net price\n",
      "\n",
      "4,49\n",
      "\n",
      "4,49\n",
      "\n",
      "5,/9\n",
      "\n",
      "Net worth\n",
      "28,24\n",
      "\n",
      "$ 28,24\n",
      "\n",
      "Net worth VAT [%]\n",
      "\n",
      "17,96 10%\n",
      "4,49 10%\n",
      "5,79 10%\n",
      "\n",
      "VAT\n",
      "2,82\n",
      "$ 2,82\n",
      "\n",
      "Gross\n",
      "worth\n",
      "\n",
      "19,76\n",
      "\n",
      "4,94\n",
      "\n",
      "6,37\n",
      "\n",
      "Gross worth\n",
      "31,06\n",
      "\n",
      "$ 31,06\n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Path to image\n",
    "image_path = \"../data/invoice_0_color_B_248.pdf0.jpg\"  # Change name if needed\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was loaded properly\n",
    "if image is None:\n",
    "    print(f\"‚ùå Error: Image not found at path: {image_path}\")\n",
    "else:\n",
    "    # Convert the image to RGB (Tesseract expects RGB format)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply OCR\n",
    "    extracted_text = pytesseract.image_to_string(image_rgb)\n",
    "\n",
    "    # Print the extracted text\n",
    "    print(\"üìÑ Extracted Invoice Text:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(extracted_text)\n",
    "    print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1274d49e-9c0b-4e4a-80c4-df7de9267988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_invoice_data(text):\n",
    "    invoice_data = {}\n",
    "\n",
    "    # 1. Extract Invoice Number\n",
    "    match = re.search(r\"Invoice\\s*no[:\\s]*([\\d\\-]+)\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        invoice_data[\"Invoice Number\"] = match.group(1)\n",
    "\n",
    "    # 2. Extract Invoice Date\n",
    "    date_pattern = r\"\\b(?:0?[1-9]|[12][0-9]|3[01])[\\/\\-](?:0?[1-9]|1[012])[\\/\\-]\\d{4}\\b\"\n",
    "    dates = re.findall(date_pattern, text)\n",
    "    if dates:\n",
    "        invoice_data[\"Invoice Date\"] = dates[0]\n",
    "\n",
    "    # 3. Extract Seller Info\n",
    "    lines = text.split(\"\\n\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"Seller\" in line:\n",
    "            seller_info_lines = []\n",
    "            j = i + 1\n",
    "            while j < len(lines) and len(seller_info_lines) < 3:\n",
    "                current_line = lines[j].strip()\n",
    "                if current_line != \"\" and not current_line.lower().startswith(\"tax id\"):\n",
    "                    seller_info_lines.append(current_line)\n",
    "                j += 1\n",
    "            invoice_data[\"Seller Info\"] = '\\n'.join(seller_info_lines)\n",
    "            break\n",
    "\n",
    "    # 4. Extract Client Info\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"Client\" in line:\n",
    "            client_info_lines = []\n",
    "            j = i + 1\n",
    "            while j < len(lines) and len(client_info_lines) < 3:\n",
    "                current_line = lines[j].strip()\n",
    "                if current_line != \"\" and not current_line.lower().startswith(\"tax id\"):\n",
    "                    client_info_lines.append(current_line)\n",
    "                j += 1\n",
    "            invoice_data[\"Client Info\"] = '\\n'.join(client_info_lines)\n",
    "            break\n",
    "\n",
    "    # 5. Extract Items\n",
    "    items = []\n",
    "    capturing_items = False\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if \"ITEMS\" in line.upper():\n",
    "            capturing_items = True\n",
    "            continue\n",
    "        if capturing_items:\n",
    "            if line == \"\" or \"SUMMARY\" in line.upper():\n",
    "                break\n",
    "            if re.match(r\"^\\d+\\.\\s\", line):  \n",
    "                items.append(line.split(\". \", 1)[1])\n",
    "            elif items:  \n",
    "                items[-1] += \" \" + line\n",
    "    if items:\n",
    "        invoice_data[\"Items\"] = items\n",
    "\n",
    "    # 6. Extract Summary Values (Net, VAT, Gross)\n",
    "    net_total_match = re.search(r\"Net worth\\s*\\$?\\s*(\\d+[\\.,]?\\d*)\", text, re.IGNORECASE)\n",
    "    vat_total_match = re.search(r\"VAT\\s*\\$?\\s*(\\d+[\\.,]?\\d*)\", text, re.IGNORECASE)\n",
    "    gross_total_match = re.search(r\"Gross worth\\s*\\$?\\s*(\\d+[\\.,]?\\d*)\", text, re.IGNORECASE)\n",
    "\n",
    "    if net_total_match:\n",
    "        invoice_data[\"Net Total\"] = net_total_match.group(1)\n",
    "    if vat_total_match:\n",
    "        invoice_data[\"VAT Total\"] = vat_total_match.group(1)\n",
    "    if gross_total_match:\n",
    "        invoice_data[\"Gross Total\"] = gross_total_match.group(1)\n",
    "\n",
    "    return invoice_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969b3f16-bccc-4f44-897b-d3208cdb564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ Final Extracted Fields:\n",
      "{'Invoice Number': '17045625', 'Invoice Date': '11/01/2017', 'Seller Info': 'Bass-Petersen\\n88013 Keith Orchard\\nPort Jenniferfurt, ID 01419', 'Client Info': 'Davidson-Martinez\\n36262 Walters Vista\\nEvansstad, ND 36644', 'Items': ['Cell: A Novel by Stephen King'], 'Net Total': '28,24', 'VAT Total': '2,82', 'Gross Total': '31,06'}\n"
     ]
    }
   ],
   "source": [
    "invoice_data = extract_invoice_data(extracted_text)\n",
    "\n",
    "print(\"\\nüßæ Final Extracted Fields:\")\n",
    "print(invoice_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb885fa5-fe57-4d41-a1b4-2ec94e1ffae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_output_schema(invoice_data):\n",
    "    # Create a clean schema\n",
    "    output_schema = {\n",
    "        \"invoice_number\": invoice_data.get(\"Invoice Number\", \"\"),\n",
    "        \"invoice_date\": invoice_data.get(\"Invoice Date\", \"\"),\n",
    "        \"seller\": {\n",
    "            \"name\": \"\",\n",
    "            \"address\": \"\"\n",
    "        },\n",
    "        \"client\": {\n",
    "            \"name\": \"\",\n",
    "            \"address\": \"\"\n",
    "        },\n",
    "        \"items\": [],\n",
    "        \"summary\": {\n",
    "            \"net_total\": invoice_data.get(\"Net Total\", \"\"),\n",
    "            \"vat_total\": invoice_data.get(\"VAT Total\", \"\"),\n",
    "            \"gross_total\": invoice_data.get(\"Gross Total\", \"\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Fill Seller Info (basic logic: first line name, rest as address)\n",
    "    if \"Seller Info\" in invoice_data:\n",
    "        seller_lines = invoice_data[\"Seller Info\"].split(\"\\n\")\n",
    "        output_schema[\"seller\"][\"name\"] = seller_lines[0] if len(seller_lines) > 0 else \"\"\n",
    "        output_schema[\"seller\"][\"address\"] = \" \".join(seller_lines[1:]) if len(seller_lines) > 1 else \"\"\n",
    "\n",
    "    # Fill Client Info (same logic)\n",
    "    if \"Client Info\" in invoice_data:\n",
    "        client_lines = invoice_data[\"Client Info\"].split(\"\\n\")\n",
    "        output_schema[\"client\"][\"name\"] = client_lines[0] if len(client_lines) > 0 else \"\"\n",
    "        output_schema[\"client\"][\"address\"] = \" \".join(client_lines[1:]) if len(client_lines) > 1 else \"\"\n",
    "\n",
    "    # Fill Items\n",
    "    if \"Items\" in invoice_data:\n",
    "        for item in invoice_data[\"Items\"]:\n",
    "            output_schema[\"items\"].append({\"description\": item})\n",
    "\n",
    "    return output_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40668197-f0c1-4ce6-85f3-5e5cbfd28e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client': {'address': '36262 Walters Vista Evansstad, ND 36644',\n",
      "            'name': 'Davidson-Martinez'},\n",
      " 'invoice_date': '11/01/2017',\n",
      " 'invoice_number': '17045625',\n",
      " 'items': [{'description': 'Cell: A Novel by Stephen King'}],\n",
      " 'seller': {'address': '88013 Keith Orchard Port Jenniferfurt, ID 01419',\n",
      "            'name': 'Bass-Petersen'},\n",
      " 'summary': {'gross_total': '31,06', 'net_total': '28,24', 'vat_total': '2,82'}}\n"
     ]
    }
   ],
   "source": [
    "final_output = transform_to_output_schema(invoice_data)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace1fb90-06a3-4671-8cae-7012532c0c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted data saved to ../output\\invoice_0.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "output_dir = \"../output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the output file path\n",
    "output_file = os.path.join(output_dir, \"invoice_0.json\")\n",
    "\n",
    "# Save the data to JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(invoice_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Extracted data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c5fe8d-8b07-43d6-833b-bd053c6031cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Client Info': 'Davidson-Martinez\\n36262 Walters Vista\\nEvansstad, ND 36644',\n",
      " 'Gross Total': '31,06',\n",
      " 'Invoice Date': '11/01/2017',\n",
      " 'Invoice Number': '17045625',\n",
      " 'Items': ['Cell: A Novel by Stephen King'],\n",
      " 'Net Total': '28,24',\n",
      " 'Seller Info': 'Bass-Petersen\\n'\n",
      "                '88013 Keith Orchard\\n'\n",
      "                'Port Jenniferfurt, ID 01419',\n",
      " 'VAT Total': '2,82'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../output/invoice_0.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d89d7-1035-468f-b94c-e73556ea05e1",
   "metadata": {},
   "source": [
    "### Code to Convert Final Output to Key-Value Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de29abf1-a1f5-47a5-a732-250fdfb56880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'key': 'invoice_number', 'value': '17045625'},\n",
      " {'key': 'invoice_date', 'value': '11/01/2017'},\n",
      " {'key': 'seller.name', 'value': 'Bass-Petersen'},\n",
      " {'key': 'seller.address',\n",
      "  'value': '88013 Keith Orchard Port Jenniferfurt, ID 01419'},\n",
      " {'key': 'client.name', 'value': 'Davidson-Martinez'},\n",
      " {'key': 'client.address', 'value': '36262 Walters Vista Evansstad, ND 36644'},\n",
      " {'key': 'items[0].description', 'value': 'Cell: A Novel by Stephen King'},\n",
      " {'key': 'summary.net_total', 'value': '28,24'},\n",
      " {'key': 'summary.vat_total', 'value': '2,82'},\n",
      " {'key': 'summary.gross_total', 'value': '31,06'}]\n"
     ]
    }
   ],
   "source": [
    "def flatten_schema_to_key_value(data, parent_key=''):\n",
    "    kv_pairs = []\n",
    "\n",
    "    for key, value in data.items():\n",
    "        full_key = f\"{parent_key}.{key}\" if parent_key else key\n",
    "\n",
    "        if isinstance(value, dict):\n",
    "            kv_pairs.extend(flatten_schema_to_key_value(value, full_key))\n",
    "        elif isinstance(value, list):\n",
    "            for i, item in enumerate(value):\n",
    "                if isinstance(item, dict):\n",
    "                    kv_pairs.extend(flatten_schema_to_key_value(item, f\"{full_key}[{i}]\"))\n",
    "                else:\n",
    "                    kv_pairs.append({\"key\": f\"{full_key}[{i}]\", \"value\": item})\n",
    "        else:\n",
    "            kv_pairs.append({\"key\": full_key, \"value\": value})\n",
    "\n",
    "    return kv_pairs\n",
    "\n",
    "# Apply to final_output\n",
    "key_value_data = flatten_schema_to_key_value(final_output)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(key_value_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b22103-0199-45c8-bc90-c8505a12c32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee127c8-4dce-4558-b07c-c67adfa23a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.3 MB 991.0 kB/s eta 0:00:26\n",
      "    --------------------------------------- 0.5/25.3 MB 6.6 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.3/25.3 MB 10.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 2.2/25.3 MB 13.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.7/25.3 MB 17.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.5/25.3 MB 20.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.2/25.3 MB 23.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.2/25.3 MB 25.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.5/25.3 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.8/25.3 MB 43.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.4/25.3 MB 46.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.9/25.3 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.5/25.3 MB 50.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.8/25.3 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 59.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "Successfully installed pyarrow-19.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Tejaswini\\anaconda3\\Lib\\site-packages\\~=arrow'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.0 requires protobuf<5,>=3.20, but you have protobuf 5.29.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall --no-cache-dir pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94e79a3-a1fe-47e3-8b8f-6ed8756bf159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'words', 'bboxes', 'ner_tags', 'image_path'],\n",
      "        num_rows: 149\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'words', 'bboxes', 'ner_tags', 'image_path'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load FUNSD dataset with permission for custom code\n",
    "dataset = load_dataset(\"nielsr/funsd\", trust_remote_code=True)\n",
    "\n",
    "# Check the structure\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27aff965-bc9c-4d89-a9c5-2e73d6585be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3Processor\n",
    "\n",
    "# Load the processor for LayoutLMv3\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c67eb0f-8d35-4960-b1e6-095d1fcd84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def preprocess_data(example):\n",
    "    # Load image\n",
    "    image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "\n",
    "    # Apply the processor\n",
    "    encoding = processor(\n",
    "        text=example[\"words\"],\n",
    "        boxes=example[\"bboxes\"],\n",
    "        word_labels=example[\"ner_tags\"],\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Flatten the result\n",
    "    encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dccb442-ff88-4122-9cec-0ce164141000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'bbox', 'labels', 'pixel_values'],\n",
       "        num_rows: 149\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'bbox', 'labels', 'pixel_values'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the preprocessing function to the dataset\n",
    "encoded_dataset = dataset.map(preprocess_data, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Peek at the features to verify\n",
    "encoded_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c991956a-6e69-4ac5-997a-83b404a1e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label list used in FUNSD\n",
    "labels = ['O', 'B-QUESTION', 'I-QUESTION', 'B-ANSWER', 'I-ANSWER', 'B-HEADER', 'I-HEADER', 'B-OTHER', 'I-OTHER']\n",
    "\n",
    "# Create label2id and id2label dictionaries\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c1b2219-ffbb-4725-8bca-b6f0dc6f1e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\",\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77487ce3-f082-4c56-95f9-79897af36865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing complete. Ready for training!\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv3Processor\n",
    "from datasets import DatasetDict\n",
    "from PIL import Image\n",
    "\n",
    "# Load processor\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "\n",
    "# Define label list from train split\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "label_list = train_dataset.features[\"ner_tags\"].feature.names\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_sample(sample):\n",
    "    image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "    \n",
    "    encoding = processor(\n",
    "        text=sample[\"words\"],\n",
    "        boxes=sample[\"bboxes\"],\n",
    "        word_labels=sample[\"ner_tags\"],\n",
    "        images=image,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Remove batch dimension\n",
    "    encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "    return encoding\n",
    "\n",
    "# Apply preprocessing\n",
    "encoded_dataset = dataset.map(preprocess_sample, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete. Ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5364804-6a1b-4061-82e0-892acf25fd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "042ab04d-43d1-41c6-beed-716a623f7af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a89c6c30-6cc8-484c-adec-cf26abf60649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.6.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tejaswini\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63e0d716-0e9c-4585-8e45-25d7b0bb26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d50314e-b23e-474a-b932-5c937f70e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./layoutlmv3-finetuned-funsd\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b71606ee-03a0-4eee-ba97-4589463c864f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tejaswini\\AppData\\Local\\Temp\\ipykernel_6280\\3025156152.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\Tejaswini\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:1575: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 28:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.332500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.860300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.829700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.787200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.580700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.849400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.535900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.9254201030731202, metrics={'train_runtime': 1724.2602, 'train_samples_per_second': 0.173, 'train_steps_per_second': 0.087, 'total_flos': 78555800254464.0, 'train_loss': 0.9254201030731202, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=processor,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84ad5396-93a2-48c3-bd1e-db63dcf6222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tejaswini\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:1575: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Predicted Tokens with Labels:\n",
      "ƒ†Inv            ‚ûú B-ANSWER\n",
      "oice            ‚ûú B-ANSWER\n",
      "ƒ†no             ‚ûú I-ANSWER\n",
      ":               ‚ûú I-ANSWER\n",
      "ƒ†Date           ‚ûú B-ANSWER\n",
      "ƒ†of             ‚ûú I-ANSWER\n",
      "ƒ†issue          ‚ûú I-ANSWER\n",
      ":               ‚ûú I-ANSWER\n",
      "ƒ†Seller         ‚ûú B-ANSWER\n",
      ":               ‚ûú I-ANSWER\n",
      "ƒ†Bass           ‚ûú B-HEADER\n",
      "-               ‚ûú I-HEADER\n",
      "P               ‚ûú I-HEADER\n",
      "eters           ‚ûú I-HEADER\n",
      "en              ‚ûú I-HEADER\n",
      "ƒ†88             ‚ûú B-HEADER\n",
      "013             ‚ûú I-HEADER\n",
      "ƒ†Keith          ‚ûú I-HEADER\n",
      "ƒ†Or             ‚ûú I-HEADER\n",
      "chard           ‚ûú I-HEADER\n",
      "ƒ†Port           ‚ûú B-HEADER\n",
      "ƒ†Jennifer       ‚ûú I-HEADER\n",
      "furt            ‚ûú I-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "ƒ†ID             ‚ûú I-HEADER\n",
      "ƒ†01             ‚ûú I-HEADER\n",
      "419             ‚ûú I-HEADER\n",
      "ƒ†Tax            ‚ûú B-ANSWER\n",
      "ƒ†Id             ‚ûú I-ANSWER\n",
      ":               ‚ûú I-ANSWER\n",
      "ƒ†9              ‚ûú B-HEADER\n",
      "03              ‚ûú I-HEADER\n",
      "-               ‚ûú I-HEADER\n",
      "94              ‚ûú I-HEADER\n",
      "-               ‚ûú I-HEADER\n",
      "76              ‚ûú I-HEADER\n",
      "10              ‚ûú I-HEADER\n",
      "ƒ†IB             ‚ûú B-HEADER\n",
      "AN              ‚ûú B-HEADER\n",
      ":               ‚ûú I-ANSWER\n",
      "ƒ†GB             ‚ûú B-HEADER\n",
      "43              ‚ûú I-HEADER\n",
      "AY              ‚ûú I-HEADER\n",
      "Y               ‚ûú I-HEADER\n",
      "U               ‚ûú I-HEADER\n",
      "13              ‚ûú I-HEADER\n",
      "39              ‚ûú I-HEADER\n",
      "240             ‚ûú I-HEADER\n",
      "474             ‚ûú I-HEADER\n",
      "27              ‚ûú I-HEADER\n",
      "25              ‚ûú I-HEADER\n",
      "ƒ†IT             ‚ûú B-ANSWER\n",
      "EMS             ‚ûú B-ANSWER\n",
      "ƒ†No             ‚ûú B-ANSWER\n",
      ".               ‚ûú B-ANSWER\n",
      "ƒ†Description    ‚ûú I-ANSWER\n",
      "ƒ†1              ‚ûú B-ANSWER\n",
      ".               ‚ûú B-ANSWER\n",
      "ƒ†Cell           ‚ûú I-ANSWER\n",
      ":               ‚ûú I-ANSWER\n",
      "ƒ†A              ‚ûú B-HEADER\n",
      "ƒ†Novel          ‚ûú I-HEADER\n",
      "ƒ†by             ‚ûú I-HEADER\n",
      "ƒ†Stephen        ‚ûú I-HEADER\n",
      "ƒ†King           ‚ûú I-HEADER\n",
      "ƒ†2              ‚ûú B-ANSWER\n",
      ".               ‚ûú B-ANSWER\n",
      "ƒ†A              ‚ûú I-ANSWER\n",
      "ƒ†History        ‚ûú I-HEADER\n",
      "ƒ†of             ‚ûú I-HEADER\n",
      "ƒ†the            ‚ûú I-HEADER\n",
      "ƒ†Indians        ‚ûú I-HEADER\n",
      "ƒ†of             ‚ûú I-HEADER\n",
      "ƒ†the            ‚ûú I-HEADER\n",
      "ƒ†United         ‚ûú I-HEADER\n",
      "ƒ†States         ‚ûú I-HEADER\n",
      "ƒ†(              ‚ûú I-HEADER\n",
      "The             ‚ûú I-HEADER\n",
      "ƒ†3              ‚ûú B-ANSWER\n",
      ".               ‚ûú B-ANSWER\n",
      "ƒ†Qu             ‚ûú I-ANSWER\n",
      "ilts            ‚ûú I-ANSWER\n",
      "ƒ†of             ‚ûú I-ANSWER\n",
      "ƒ†Illusion       ‚ûú I-ANSWER\n",
      "ƒ†SUM            ‚ûú B-ANSWER\n",
      "M               ‚ûú B-ANSWER\n",
      "ARY             ‚ûú I-ANSWER\n",
      "ƒ†Total          ‚ûú B-ANSWER\n",
      "2017            ‚ûú I-HEADER\n",
      "ƒ†Q              ‚ûú B-ANSWER\n",
      "ty              ‚ûú I-ANSWER\n",
      "ƒ†u              ‚ûú I-ANSWER\n",
      "M               ‚ûú I-ANSWER\n",
      "ƒ†4              ‚ûú B-HEADER\n",
      ",               ‚ûú B-HEADER\n",
      "00              ‚ûú I-HEADER\n",
      "ƒ†each           ‚ûú I-HEADER\n",
      "ƒ†1              ‚ûú B-HEADER\n",
      ",               ‚ûú B-HEADER\n",
      "00              ‚ûú I-HEADER\n",
      "ƒ†each           ‚ûú I-HEADER\n",
      "ƒ†1              ‚ûú B-HEADER\n",
      ",               ‚ûú B-HEADER\n",
      "00              ‚ûú I-HEADER\n",
      "ƒ†each           ‚ûú I-HEADER\n",
      "ƒ†VAT            ‚ûú B-ANSWER\n",
      "ƒ†[              ‚ûú I-ANSWER\n",
      "%]              ‚ûú I-ANSWER\n",
      "ƒ†10             ‚ûú B-HEADER\n",
      "%               ‚ûú B-HEADER\n",
      "ƒ†Client         ‚ûú B-ANSWER\n",
      ":               ‚ûú I-ANSWER\n",
      "ƒ†Davidson       ‚ûú B-HEADER\n",
      "-               ‚ûú I-HEADER\n",
      "Mart            ‚ûú I-HEADER\n",
      "inez            ‚ûú I-HEADER\n",
      "ƒ†36             ‚ûú B-HEADER\n",
      "262             ‚ûú I-HEADER\n",
      "ƒ†Walters        ‚ûú I-HEADER\n",
      "ƒ†Vista          ‚ûú I-HEADER\n",
      "ƒ†Evans          ‚ûú I-HEADER\n",
      "stad            ‚ûú I-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "ƒ†ND             ‚ûú I-HEADER\n",
      "ƒ†36             ‚ûú I-HEADER\n",
      "644             ‚ûú I-HEADER\n",
      "ƒ†Tax            ‚ûú B-ANSWER\n",
      "ƒ†Id             ‚ûú I-ANSWER\n",
      ":               ‚ûú I-ANSWER\n",
      "ƒ†9              ‚ûú B-HEADER\n",
      "13              ‚ûú I-HEADER\n",
      "-               ‚ûú I-HEADER\n",
      "98              ‚ûú I-HEADER\n",
      "-               ‚ûú I-HEADER\n",
      "26              ‚ûú I-HEADER\n",
      "20              ‚ûú I-HEADER\n",
      "ƒ†Net            ‚ûú B-ANSWER\n",
      "ƒ†price          ‚ûú I-ANSWER\n",
      "ƒ†4              ‚ûú B-HEADER\n",
      ",               ‚ûú B-HEADER\n",
      "49              ‚ûú I-HEADER\n",
      "ƒ†4              ‚ûú B-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "49              ‚ûú I-HEADER\n",
      "ƒ†5              ‚ûú B-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "/               ‚ûú I-HEADER\n",
      "9               ‚ûú I-HEADER\n",
      "ƒ†Net            ‚ûú B-ANSWER\n",
      "ƒ†worth          ‚ûú I-ANSWER\n",
      "ƒ†28             ‚ûú B-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "24              ‚ûú I-HEADER\n",
      "ƒ†$              ‚ûú B-HEADER\n",
      "ƒ†28             ‚ûú I-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "24              ‚ûú I-HEADER\n",
      "ƒ†Net            ‚ûú B-ANSWER\n",
      "ƒ†worth          ‚ûú I-ANSWER\n",
      "ƒ†VAT            ‚ûú I-ANSWER\n",
      "ƒ†[              ‚ûú I-ANSWER\n",
      "%]              ‚ûú I-ANSWER\n",
      "ƒ†17             ‚ûú B-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "96              ‚ûú I-HEADER\n",
      "ƒ†10             ‚ûú B-HEADER\n",
      "%               ‚ûú I-HEADER\n",
      "ƒ†4              ‚ûú B-HEADER\n",
      ",               ‚ûú B-HEADER\n",
      "49              ‚ûú I-HEADER\n",
      "ƒ†10             ‚ûú B-HEADER\n",
      "%               ‚ûú I-HEADER\n",
      "ƒ†5              ‚ûú B-HEADER\n",
      ",               ‚ûú B-HEADER\n",
      "79              ‚ûú I-HEADER\n",
      "ƒ†10             ‚ûú B-HEADER\n",
      "%               ‚ûú I-HEADER\n",
      "ƒ†VAT            ‚ûú B-ANSWER\n",
      "ƒ†2              ‚ûú B-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "82              ‚ûú I-HEADER\n",
      "ƒ†$              ‚ûú B-HEADER\n",
      "ƒ†2              ‚ûú I-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "82              ‚ûú I-HEADER\n",
      "ƒ†Gross          ‚ûú B-ANSWER\n",
      "ƒ†worth          ‚ûú I-ANSWER\n",
      "ƒ†19             ‚ûú B-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "76              ‚ûú I-HEADER\n",
      "ƒ†4              ‚ûú B-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "94              ‚ûú I-HEADER\n",
      "ƒ†6              ‚ûú B-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "37              ‚ûú I-HEADER\n",
      "ƒ†Gross          ‚ûú B-ANSWER\n",
      "ƒ†worth          ‚ûú I-ANSWER\n",
      "ƒ†31             ‚ûú B-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "06              ‚ûú I-HEADER\n",
      "ƒ†$              ‚ûú B-HEADER\n",
      "ƒ†31             ‚ûú I-HEADER\n",
      ",               ‚ûú I-HEADER\n",
      "06              ‚ûú I-HEADER\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import torch\n",
    "\n",
    "# Load invoice image\n",
    "image_path = \"../data/invoice_0_color_B_248.pdf0.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Apply OCR using pytesseract\n",
    "ocr_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "\n",
    "words = []\n",
    "boxes = []\n",
    "\n",
    "for i in range(len(ocr_data[\"text\"])):\n",
    "    word = ocr_data[\"text\"][i]\n",
    "    if word.strip() != \"\":\n",
    "        words.append(word)\n",
    "\n",
    "        # Get box and normalize to 0-1000 scale\n",
    "        x, y, w, h = (\n",
    "            ocr_data[\"left\"][i],\n",
    "            ocr_data[\"top\"][i],\n",
    "            ocr_data[\"width\"][i],\n",
    "            ocr_data[\"height\"][i],\n",
    "        )\n",
    "\n",
    "        x1 = int(x * 1000 / image.width)\n",
    "        y1 = int(y * 1000 / image.height)\n",
    "        x2 = int((x + w) * 1000 / image.width)\n",
    "        y2 = int((y + h) * 1000 / image.height)\n",
    "\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "# Prepare input for the model\n",
    "encoding = processor(\n",
    "    images=image,\n",
    "    text=words,\n",
    "    boxes=boxes,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Move to eval mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)\n",
    "\n",
    "# Decode predictions\n",
    "pred_ids = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "input_ids = encoding[\"input_ids\"].squeeze().tolist()\n",
    "\n",
    "tokens = processor.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "labels = [id2label[pred] for pred in pred_ids]\n",
    "\n",
    "# Show only meaningful predictions\n",
    "print(\"\\nüîç Predicted Tokens with Labels:\")\n",
    "for token, label in zip(tokens, labels):\n",
    "    if label != \"O\":\n",
    "        print(f\"{token:<15} ‚ûú {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "996d53a1-9d8c-4907-855d-b923ed2c205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ Final Extracted Fields:\n",
      "answer: Inv oice no : Date of issue : Seller : Tax Id : IT EMS No . Description 1 . Cell : 2 . A History of the Indians of the United States ( The 3 . Qu ilts of Illusion SUM M ARY Total Q ty u M VAT [ %] Client : Tax Id : Net price Net worth Net worth VAT [ %] VAT Gross worth Gross worth\n",
      "header: Bass - P eters en 88 013 Keith Or chard Port Jennifer furt , ID 01 419 9 03 - 94 - 76 10 IB AN : GB 43 AY Y U 13 39 240 474 27 25 A Novel by Stephen King 4 , 00 each 1 , 00 each 1 , 00 each 10 % Davidson - Mart inez 36 262 Walters Vista Evans stad , ND 36 644 9 13 - 98 - 26 20 4 , 49 4 , 49 5 , / 9 28 , 24 $ 28 , 24 17 , 96 10 % 4 , 49 10 % 5 , 79 10 % 2 , 82 $ 2 , 82 19 , 76 4 , 94 6 , 37 31 , 06 $ 31 , 06\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Combine tokens and their labels\n",
    "extracted_fields = defaultdict(str)\n",
    "current_label = None\n",
    "\n",
    "for token, label in zip(tokens, labels):\n",
    "    if label.startswith(\"B-\"):\n",
    "        current_label = label[2:].lower()\n",
    "        if current_label not in extracted_fields:\n",
    "            extracted_fields[current_label] = token\n",
    "        else:\n",
    "            extracted_fields[current_label] += \" \" + token\n",
    "    elif label.startswith(\"I-\") and current_label:\n",
    "        extracted_fields[current_label] += \" \" + token\n",
    "    else:\n",
    "        current_label = None\n",
    "\n",
    "# Clean up tokens (remove extra ƒ† from WordPiece tokens)\n",
    "for key in extracted_fields:\n",
    "    extracted_fields[key] = extracted_fields[key].replace(\"ƒ†\", \"\").strip()\n",
    "\n",
    "# Print final structured output\n",
    "print(\"\\nüßæ Final Extracted Fields:\")\n",
    "for k, v in extracted_fields.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e59440d-fac8-451b-946a-5db33df4a221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Output saved to final_invoice_output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"final_invoice_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dict(extracted_fields), f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Output saved to final_invoice_output.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "69e652cc-8dfe-4ad7-bb4f-866a1187c4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e62ea_row0_col0, #T_e62ea_row0_col1, #T_e62ea_row1_col0, #T_e62ea_row1_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e62ea\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e62ea_level0_col0\" class=\"col_heading level0 col0\" >Field</th>\n",
       "      <th id=\"T_e62ea_level0_col1\" class=\"col_heading level0 col1\" >Extracted Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e62ea_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e62ea_row0_col0\" class=\"data row0 col0\" >answer</td>\n",
       "      <td id=\"T_e62ea_row0_col1\" class=\"data row0 col1\" >Inv oice no : Date of issue : Seller : Tax Id : IT EMS No . Description 1 . Cell : 2 . A History of the Indians of the United States ( The 3 . Qu ilts of Illusion SUM M ARY Total Q ty u M VAT [ %] Client : Tax Id : Net price Net worth Net worth VAT [ %] VAT Gross worth Gross worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e62ea_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e62ea_row1_col0\" class=\"data row1 col0\" >header</td>\n",
       "      <td id=\"T_e62ea_row1_col1\" class=\"data row1 col1\" >Bass - P eters en 88 013 Keith Or chard Port Jennifer furt , ID 01 419 9 03 - 94 - 76 10 IB AN : GB 43 AY Y U 13 39 240 474 27 25 A Novel by Stephen King 4 , 00 each 1 , 00 each 1 , 00 each 10 % Davidson - Mart inez 36 262 Walters Vista Evans stad , ND 36 644 9 13 - 98 - 26 20 4 , 49 4 , 49 5 , / 9 28 , 24 $ 28 , 24 17 , 96 10 % 4 , 49 10 % 5 , 79 10 % 2 , 82 $ 2 , 82 19 , 76 4 , 94 6 , 37 31 , 06 $ 31 , 06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19d8eb26120>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the final JSON output\n",
    "with open(\"final_invoice_output.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to a table-friendly format\n",
    "table_data = [{\"Field\": key, \"Extracted Value\": value} for key, value in data.items()]\n",
    "\n",
    "# Display as a table using pandas\n",
    "df = pd.DataFrame(table_data)\n",
    "df.style.set_properties(**{'text-align': 'left'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f73fc578-6c1f-43db-8773-37ea4b1f3506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field</th>\n",
       "      <th>Extracted Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invoice Number</td>\n",
       "      <td>88 013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>Date not clearly found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seller</td>\n",
       "      <td>Keith Or chard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tax ID</td>\n",
       "      <td>01 419 9 03 - 94 - 76 10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Field           Extracted Value\n",
       "0  Invoice Number                    88 013\n",
       "1            Date    Date not clearly found\n",
       "2          Seller            Keith Or chard\n",
       "3          Tax ID  01 419 9 03 - 94 - 76 10"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# This is full header text (copied from output)\n",
    "header_text = \"\"\"Bass - P eters en 88 013 Keith Or chard Port Jennifer furt , ID 01 419 9 03 - 94 - 76 10 IB AN : GB 43 AY Y U 13 39 240 474 27 25 A Novel by Stephen King 4 , 00 each 1 , 00 each 1 , 00 each 10 % Davidson - Mart inez 36 262 Walters Vista Evans stad , ND 36 644 9 13 - 98 - 26 20 4 , 49 4 , 49 5 , / 9 28 , 24 2 , 82 19 , 76 4 , 94 6 , 37 31 , 06 $ 31 , 06\"\"\"\n",
    "\n",
    "# Clean the messy spacing \n",
    "header_text = header_text.replace(\"  \", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "# Extract values using simple regex patterns\n",
    "invoice_match = re.search(r'\\b\\d{2}\\s?\\d{3}\\b', header_text)  # like '88 013'\n",
    "seller_match = re.search(r'\\d{2}\\s?\\d{3}\\s(.*?)\\sPort', header_text)  # like 'Keith Orchard'\n",
    "taxid_match = re.search(r'ID\\s+(\\d{2,3}.*?)IB', header_text)  # between ID and IBAN\n",
    "\n",
    "# Dummy date \n",
    "date = \"Date not clearly found\"\n",
    "\n",
    "# Store results\n",
    "fields = {\n",
    "    \"Invoice Number\": invoice_match.group(0) if invoice_match else \"Not found\",\n",
    "    \"Date\": date,\n",
    "    \"Seller\": seller_match.group(1).strip() if seller_match else \"Not found\",\n",
    "    \"Tax ID\": taxid_match.group(1).strip() if taxid_match else \"Not found\"\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(list(fields.items()), columns=[\"Field\", \"Extracted Value\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2a19a4b9-b412-4dc6-8c82-56156b8f59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_match = re.search(r'\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{2,4}', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "24e541c0-38c2-4cd9-b41e-bd67142be89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"invoice_fields.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280a969-7585-4aea-836e-dd2141ffc061",
   "metadata": {},
   "source": [
    "# üßæ Final Project Summary: Invoice Information Extraction\r\n",
    "\r\n",
    "## üìå Objective\r\n",
    "To extract structured information such as **Invoice Number**, **Invoice Date**, **Seller**, **Client**, **Line Items**, and **Totals** from invoice images using a combination of **OCR** and **deep learning (LayoutLMv3)**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## ‚úÖ Steps Followed\r\n",
    "\r\n",
    "### 1. OCR-Based Pipeline (Rule-Based)\r\n",
    "- Used **Tesseract OCR** to extract raw text from invoice images.\r\n",
    "- Applied **Python regex** and line-based parsing to extract:\r\n",
    "  - Invoice Number\r\n",
    "  - Invoice Date\r\n",
    "  - Seller and Client Info\r\n",
    "  - Items and Totals\r\n",
    "- Saved the output to `final_invoice_output.json`.\r\n",
    "\r\n",
    "### 2. LayoutLMv3 Model Training (FUNSD Dataset)\r\n",
    "- Loaded the **FUNSD** dataset from Hugging Face.\r\n",
    "- Fine-tuned the **LayoutLMv3** transformer model for document key-value extraction.\r\n",
    "- Preprocessed words and bounding boxes, and trained using Hugging Face `Trainer`.\r\n",
    "- Ran inference on a real invoice image to test scalability.\r\n",
    "\r\n",
    "### 3. Scalable Pipeline\r\n",
    "- Designed the structure to support **training for any custom field**.\r\n",
    "- Model + OCR logic can be extended to forms, receipts, IDs, and contracts.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## üõ† Tools & Libraries\r\n",
    "- **Tesseract OCR**\r\n",
    "- **Hugging Face Transformers & Datasets**\r\n",
    "- **LayoutLMv3** (`microsoft/layoutlmv3-base`)\r\n",
    "- **FUNSD Dataset**\r\n",
    "- Python (Pandas, Regex, Torch, JSON)\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## üì¶ Final Output\r\n",
    "\r\n",
    "Sample extracted fields (saved in JSON):\r\n",
    "\r\n",
    "```json\r\n",
    "{\r\n",
    "  \"invoice_number\": \"17045625\",\r\n",
    "  \"invoice_date\": \"11/01/2017\",\r\n",
    "  \"seller_name\": \"Bass-Petersen\",\r\n",
    "  \"client_name\": \"Davidson-Martinez\",\r\n",
    "  \"gross_total\": \"31.06\",\r\n",
    "  \"vat_total\": \"2.82\"\r\n",
    "}\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7674ed9-aaf2-4e8d-ac84-2909629e4346",
   "metadata": {},
   "source": [
    "> **Note:** Since FUNSD has general tags like `'header'` and `'answer'`, the model was able to identify form blocks but not field-specific values.  \r\n",
    "> To extract fields like `invoice_number` or `gross_total`, we combined this with rule-based extraction using OCR.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f4fee-7bd2-4781-acfe-e31f7d5ef6b6",
   "metadata": {},
   "source": [
    "## üßæ Final Cleaned Output (From OCR)\n",
    "\n",
    "| Field           | Extracted Value                                           |\n",
    "|------------------|-----------------------------------------------------------|\n",
    "| Invoice Number   | 17045625                                                  |\n",
    "| Invoice Date     | 11/01/2017                                                |\n",
    "| Seller Name      | Bass-Petersen                                             |\n",
    "| Seller Address   | 88013 Keith Orchard, Port Jenniferfurt, ID 01419         |\n",
    "| Client Name      | Davidson-Martinez                                         |\n",
    "| Client Address   | 36262 Walters Vista, Evansstad, ND 36644                 |\n",
    "| Line Items       | 1. Cell: A Novel by Stephen King  <br> 2. A History of the Indians of the United States (The) <br> 3. Quilts of Illusion |\n",
    "| Net Total        | 28,24                                                     |\n",
    "| VAT Total        | 2,82                                                      |\n",
    "| Gross Total      | 31,06                                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd0150-e1bc-4e3b-8655-403ce1640694",
   "metadata": {},
   "source": [
    "> Extracted using Tesseract OCR + rule-based logic (Python regex & line matching)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd3f4a-e24b-4376-8b8b-ca30eb279648",
   "metadata": {},
   "source": [
    "## üéØ Result\n",
    "\n",
    "Successfully implemented an end-to-end invoice information extraction system using both rule-based and deep learning approaches.\n",
    "\n",
    "The solution is:\n",
    "- ‚úÖ Modular\n",
    "- ‚úÖ Scalable\n",
    "- ‚úÖ Ready for real-world applications\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
